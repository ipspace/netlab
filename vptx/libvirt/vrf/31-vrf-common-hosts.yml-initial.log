[WARNING]: Could not match supplied host pattern, ignoring: unprovisioned

PLAY [Deploy initial device configuration] *************************************

TASK [Set variables that cannot be set with VARS] ******************************
ok: [h1]
ok: [h2]
ok: [srv]
ok: [dut]

TASK [Find device readiness script] ********************************************
ok: [h1]
ok: [h2]
ok: [srv]
ok: [dut]

TASK [Wait for device to become ready] *****************************************
skipping: [h1]
skipping: [h2]
skipping: [srv]
included: /home/pipi/net101/tools/netsim/ansible/tasks/readiness-check/vptx.yml for dut

TASK [Wait for et-0/0/1 to appear] *********************************************
ok: [dut]

TASK [Deploy initial configuration] ********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for h1, h2, srv, dut

TASK [Figure out whether to deploy the module initial on current device] *******
ok: [h1]
ok: [h2]
ok: [srv]
ok: [dut]

TASK [Find configuration template for initial] *********************************
ok: [h1]
ok: [h2]
ok: [srv]
ok: [dut]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [h1] => 
  msg: |-
    initial configuration for h1
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.0.1/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.0.1/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.0.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.0.4
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.0.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.0.4
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.0.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.0.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.0.4
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.0.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.0.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.0.4
    #
    # Print the final routing table
    ip route
ok: [h2] => 
  msg: |-
    initial configuration for h2
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.1.2/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.1.2/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.1.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.1.4
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.1.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.1.4
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.1.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.1.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.1.4
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.1.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.1.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.1.4
    #
    # Print the final routing table
    ip route
ok: [srv] => 
  msg: |-
    initial configuration for srv
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.2.3/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.2.3/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.2.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.2.4
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.2.4
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.2.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.2.4
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.2.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.2.4
    #
    # Print the final routing table
    ip route
ok: [dut] => 
  msg: |-
    initial configuration for dut
    =========================================
    system {
      host-name dut
        static-host-mapping {
            h1 inet 172.16.0.1;
            h2 inet 172.16.1.2;
            srv inet 172.16.2.3;
        }
    }
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    policy-options {
      community tg_65000_2 members target:65000:2;
      community tg_65000_3 members target:65000:3;
      community tg_65000_1 members target:65000:1;
    }
  
  
  
    policy-options {
      policy-statement vrf-blue-export {
        term 1 {
          then {
            community add tg_65000_2;
            accept;
          }
        }
      }
  
  
  
  
      policy-statement vrf-blue-import {
        term 1 {
          from community [ tg_65000_2 tg_65000_3 ];
          then accept;
        }
        term default {
          then reject;
        }
      }
      policy-statement vrf-common-export {
        term 1 {
          then {
            community add tg_65000_3;
            accept;
          }
        }
      }
  
  
  
  
  
      policy-statement vrf-common-import {
        term 1 {
          from community [ tg_65000_1 tg_65000_2 tg_65000_3 ];
          then accept;
        }
        term default {
          then reject;
        }
      }
      policy-statement vrf-red-export {
        term 1 {
          then {
            community add tg_65000_1;
            accept;
          }
        }
      }
  
  
  
  
      policy-statement vrf-red-import {
        term 1 {
          from community [ tg_65000_1 tg_65000_3 ];
          then accept;
        }
        term default {
          then reject;
        }
      }
    }
  
    routing-instances {
  
      blue {
        instance-type vrf;
        route-distinguisher 65000:2;
  
        vrf-import vrf-blue-import;
        vrf-export vrf-blue-export;
  
        routing-options {
          auto-export;
        }
  
        interface et-0/0/1.0;
  
      }
  
  
      common {
        instance-type vrf;
        route-distinguisher 65000:3;
  
        vrf-import vrf-common-import;
        vrf-export vrf-common-export;
  
        routing-options {
          auto-export;
        }
  
        interface et-0/0/2.0;
  
      }
  
  
      red {
        instance-type vrf;
        route-distinguisher 65000:1;
  
        vrf-import vrf-red-import;
        vrf-export vrf-red-export;
  
        routing-options {
          auto-export;
        }
  
        interface et-0/0/0.0;
  
      }
  
    }
    interfaces {
  
      lo0.0 {
  
          family inet {
            address 10.0.0.4/32;
          }
  
      }
      et-0/0/0.0 {
        description "dut -> h1 [stub]";
  
          family inet {
            address 172.16.0.4/24;
          }
  
      }
      et-0/0/1.0 {
        description "dut -> h2 [stub]";
  
          family inet {
            address 172.16.1.4/24;
          }
  
      }
      et-0/0/2.0 {
        description "dut -> srv [stub]";
  
          family inet {
            address 172.16.2.4/24;
          }
  
      }
    }
    protocols {
      lldp {
        interface re0:mgmt-0 {
          disable;
        }
        interface all;
      }
    }

TASK [Find configuration deployment deploy_script for initial] *****************
ok: [h1]
ok: [h2]
ok: [srv]
ok: [dut]

TASK [Deploy initial configuration] ********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/linux/initial-clab.yml for h1, h2, srv
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/junos.yml for dut

TASK [set_fact] ****************************************************************
ok: [h1]
ok: [h2]
ok: [srv]

TASK [Create initial container setup from /home/pipi/net101/tools/netsim/ansible/templates/initial/linux-clab.j2] ***
changed: [h1 -> localhost]
changed: [srv -> localhost]
changed: [h2 -> localhost]

TASK [Initial container configuration via /tmp/config-erHqbjqM-h1.sh] **********
changed: [h1 -> localhost]
changed: [srv -> localhost]
changed: [h2 -> localhost]

TASK [file] ********************************************************************
changed: [srv -> localhost]
changed: [h2 -> localhost]
changed: [h1 -> localhost]

TASK [junos_config: deploying initial from /home/pipi/net101/tools/netsim/ansible/templates/initial/junos.j2] ***
changed: [dut]

PLAY [Deploy module-specific configurations] ***********************************

TASK [Set variables that cannot be set with VARS] ******************************
ok: [dut]

TASK [Deploy individual configuration modules] *********************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for dut => (item=vrf)

TASK [Figure out whether to deploy the module vrf on current device] ***********
ok: [dut]

TASK [Find configuration template for vrf] *************************************
ok: [dut]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [dut] => 
  msg: |-
    vrf configuration for dut
    =========================================
  
  
    policy-options {
  
      policy-statement vrf-blue-ibgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
      policy-statement vrf-blue-ebgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
  
      policy-statement vrf-common-ibgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
      policy-statement vrf-common-ebgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
  
      policy-statement vrf-red-ibgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
      policy-statement vrf-red-ebgp-export {
        term redis_direct {
          from {
            protocol direct;
          }
          then accept;
        }
        term redis_ospf {
          from {
            protocol ospf;
          }
          then accept;
        }
        term redis_ospf3 {
          from {
            protocol ospf3;
          }
          then accept;
        }
      }
  
    }
  
  
    routing-instances {
  
      blue {
        routing-options {
          autonomous-system 65000;
          router-id 10.0.0.4
        }
  
        protocols {
          bgp {
  
            group ebgp-peers {
              export vrf-blue-ebgp-export;
              advertise-inactive;
            }
          }
        }
      }
  
  
      common {
        routing-options {
          autonomous-system 65000;
          router-id 10.0.0.4
        }
  
        protocols {
          bgp {
  
            group ebgp-peers {
              export vrf-common-ebgp-export;
              advertise-inactive;
            }
          }
        }
      }
  
  
      red {
        routing-options {
          autonomous-system 65000;
          router-id 10.0.0.4
        }
  
        protocols {
          bgp {
  
            group ebgp-peers {
              export vrf-red-ebgp-export;
              advertise-inactive;
            }
          }
        }
      }
  
    }

TASK [Find configuration deployment deploy_script for vrf] *********************
ok: [dut]

TASK [Deploy vrf configuration] ************************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/junos.yml for dut

TASK [junos_config: deploying vrf from /home/pipi/net101/tools/netsim/ansible/templates/vrf/junos.j2] ***
changed: [dut]

PLAY [Deploy custom deployment templates] **************************************
skipping: no hosts matched

PLAY RECAP *********************************************************************
dut                        : ok=19   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
h1                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
h2                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
srv                        : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   



The device under test has two user VRFs and a common services VRF. The
lab tests inter-VRF route leaking between common VRF and other VRFs

* h1 and h2 should be able to ping srv but not each other

