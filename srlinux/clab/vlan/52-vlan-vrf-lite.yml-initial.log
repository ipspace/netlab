[WARNING]: Could not match supplied host pattern, ignoring: unprovisioned

PLAY [Deploy initial device configuration] *************************************

TASK [Set variables that cannot be set with VARS] ******************************
ok: [r2]
ok: [h1]
ok: [h2]
ok: [h3]
ok: [r1]
ok: [h4]
ok: [h5]
ok: [r3]

TASK [Find device readiness script] ********************************************
ok: [r2]
ok: [h1]
ok: [h2]
ok: [h3]
ok: [r1]
ok: [h4]
ok: [h5]
ok: [r3]

TASK [Wait for device to become ready] *****************************************
skipping: [r2]
skipping: [h1]
skipping: [h2]
skipping: [h3]
skipping: [h4]
skipping: [h5]
skipping: [r1]
skipping: [r3]

TASK [Deploy initial configuration] ********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for r2, h1, h2, h3, h4, h5, r1, r3

TASK [Figure out whether to deploy the module initial on current device] *******
ok: [r2]
ok: [h1]
ok: [h2]
ok: [h4]
ok: [h3]
ok: [h5]
ok: [r1]
ok: [r3]

TASK [Find configuration template for initial] *********************************
ok: [r2]
ok: [h1]
ok: [h2]
ok: [h3]
ok: [h4]
ok: [h5]
ok: [r1]
ok: [r3]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [r2] => 
  msg: |-
    initial configuration for r2
    =========================================
    #!/bin/bash
    #
    set -e
    set -x
    #
    # Create bash profile script
    #
    cat <<SCRIPT >/root/.bash_profile
    #!/bin/bash
    #
    export PS1="\h(bash)#"
    echo "Use vtysh to connect to FRR daemon"
    echo
    SCRIPT
    #
    # Get the current next hop for the default route
    #
    def_nh=$(ip route list default|awk '{ print $3 }')
    #
    # Create the management VRF and add eth0 to it
    #
    if [ ! -e /sys/devices/virtual/net/mgmt ]; then
      ip link add mgmt type vrf table 42
    fi
    ip link set mgmt up
    sysctl -qw net.ipv6.conf.eth0.keep_addr_on_down=1
    ip link set eth0 master mgmt
    #
    # Reinstall the default route if we had it before
    #
    if [[ -n "$def_nh" ]]; then
      ip route add 0.0.0.0/0 vrf mgmt via $def_nh
    fi
    #
    # Enable FRR modules (if not using containerlab bind-mounted /etc/frr/daemons)
    #
  
    #
    # Create loopbacks and stub devices
    #
    if [ ! -e /sys/class/net/lo ]; then
      if [ ! -e /sys/devices/virtual/net/lo ]; then
        ip link add lo type dummy
        ip link set dev lo up
      fi
    fi
  
    # Disable IPv6 (for IPv4-only interfaces) or SLAAC (if the device is a router)
    #
    sysctl -qw net.ipv6.conf.eth1.disable_ipv6=1
    sysctl -qw net.ipv6.conf.eth2.disable_ipv6=1
  
    #
    # Add vtysh.conf file
    echo "service integrated-vtysh-config" >/etc/frr/vtysh.conf
    #
    # Set Ethernet interface MTU
    ip link set eth1 mtu 1500
    ip link set eth2 mtu 1500
    #
    # Rest of initial configuration done through VTYSH
    #
    cat >/tmp/config <<CONFIG
    hostname r2
    !
    vrf mgmt
     exit-vrf
    !
    frr defaults datacenter
    !
    interface lo
     no shutdown
     ip address 10.0.0.2/32
    !
    interface eth1
     no shutdown
     description r2 -> r1
     ! no ip address
    !
    interface eth2
     no shutdown
     description r2 -> r3
     ! no ip address
    !
    interface eth1.701
     no shutdown
     description r2 -> r1
     ip address 172.16.7.2/24
    !
    interface eth1.700
     no shutdown
     description r2 -> r1
     ip address 172.16.8.2/24
    !
    interface eth2.701
     no shutdown
     description r2 -> r3
     ip address 172.16.9.2/24
    !
    interface eth2.700
     no shutdown
     description r2 -> r3
     ip address 172.16.10.2/24
    !
    do write
    CONFIG
    vtysh -f /tmp/config
    exit 0
ok: [h1] => 
  msg: |-
    initial configuration for h1
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.2.4/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.2.4/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.2.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.2.1
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.2.1
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.2.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.2.1
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.2.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.2.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.2.1
    #
    # Print the final routing table
    ip route
ok: [h2] => 
  msg: |-
    initial configuration for h2
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.3.5/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.3.5/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.3.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.3.3
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.3.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.3.3
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.3.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.3.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.3.3
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.3.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.3.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.3.3
    #
    # Print the final routing table
    ip route
ok: [h4] => 
  msg: |-
    initial configuration for h4
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.6.7/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.6.7/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.6.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.6.3
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.6.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.6.3
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.6.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.6.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.6.3
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.6.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.6.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.6.3
    #
    # Print the final routing table
    ip route
ok: [r1] => 
  msg: |-
    initial configuration for r1
    =========================================
  
    updates:
  
    - path: /interface[name=system0]/subinterface[index=0]
      value:
       description: "No description"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "10.0.0.1/32"
  
  
  
  
    - path: /network-instance[name=default]
      value:
       type: default
       interface:
       - name: system0.0
  
  
  
  
    # TODO: vdata.rd, vdata.import/export, vdata.af
  
  
  
    # TODO: vdata.rd, vdata.import/export, vdata.af
ok: [r3] => 
  msg: |-
    initial configuration for r3
    =========================================
  
    updates:
  
    - path: /interface[name=system0]/subinterface[index=0]
      value:
       description: "No description"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "10.0.0.3/32"
  
  
  
  
    - path: /network-instance[name=default]
      value:
       type: default
       interface:
       - name: system0.0
  
  
  
  
    # TODO: vdata.rd, vdata.import/export, vdata.af
  
  
  
    # TODO: vdata.rd, vdata.import/export, vdata.af
ok: [h3] => 
  msg: |-
    initial configuration for h3
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.5.6/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.5.6/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.5.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.5.1
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.5.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.5.1
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.5.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.5.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.5.1
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.5.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.5.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.5.1
    #
    # Print the final routing table
    ip route
ok: [h5] => 
  msg: |-
    initial configuration for h5
    =========================================
    #!/bin/bash
    #
    # This script contains the 'ip' commands needed to set up container
    # interfaces and route table. It's executed within the container
    # network namespace on the container host.
    #
    #  /etc/hosts file is generated as a clab bind.
    #
    set -e
    ### One-Shot configuration (non-Ubuntu VM or container)
    #
    # Disable IPv4 and IPv6 forwarding
    #
    sysctl -w net.ipv4.ip_forward=0
    sysctl -w net.ipv6.conf.all.forwarding=0
    #
    # Interface addressing
    #
    ip link set dev eth1 up
    set +e
    ip addr del 172.16.4.8/24 dev eth1 2>/dev/null
    set -e
    ip addr add 172.16.4.8/24 dev eth1
    ip link set eth1 mtu 1500
    #
    # Add routes to IPv4 address pools pointing to the first neighbor on the first link
    #
    # If you need anything better, use FRR instead of Linux and start routing (or use IPv6)
    #
    # lan prefix: 172.16.0.0/16 local subnet: 172.16.4.0/24
    set +e
    ip route del 172.16.0.0/16 2>/dev/null
    set -e
    ip route add 172.16.0.0/16 via 172.16.4.1
    # loopback prefix: 10.0.0.0/24 local subnet: 172.16.4.0/24
    set +e
    ip route del 10.0.0.0/24 2>/dev/null
    set -e
    ip route add 10.0.0.0/24 via 172.16.4.1
    # mgmt prefix: 192.168.17.0/24 local subnet: 172.16.4.0/24
    # p2p prefix: 10.1.0.0/16 local subnet: 172.16.4.0/24
    set +e
    ip route del 10.1.0.0/16 2>/dev/null
    set -e
    ip route add 10.1.0.0/16 via 172.16.4.1
    # router_id prefix: 10.0.0.0/24 local subnet: 172.16.4.0/24
    # vrf_loopback prefix: 10.2.0.0/24 local subnet: 172.16.4.0/24
    set +e
    ip route del 10.2.0.0/24 2>/dev/null
    set -e
    ip route add 10.2.0.0/24 via 172.16.4.1
    #
    # Print the final routing table
    ip route

TASK [Find configuration deployment deploy_script for initial] *****************
ok: [r2]
ok: [h1]
ok: [h2]
ok: [h3]
ok: [r1]
ok: [h4]
ok: [r3]
ok: [h5]

TASK [Deploy initial configuration] ********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/frr/initial-clab.yml for r2
included: /home/pipi/net101/tools/netsim/ansible/tasks/linux/initial-clab.yml for h1, h2, h3, h4, h5
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/srlinux.yml for r1, r3

TASK [Attempt to load VRF kernel module] ***************************************
changed: [r2 -> localhost]

TASK [Disable FRR management VRF when modprobe fails] **************************
skipping: [r2]

TASK [include_tasks] ***********************************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/frr/deploy-config.yml for r2

TASK [template] ****************************************************************
changed: [r2]

TASK [set_fact] ****************************************************************
ok: [r2]

TASK [run /tmp/config.sh to deploy initial config from /home/pipi/net101/tools/netsim/ansible/templates/initial/frr.j2] ***
changed: [r2]

TASK [run vtysh to import initial config from /home/pipi/net101/tools/netsim/ansible/templates/initial/frr.j2] ***
skipping: [r2]

TASK [set_fact] ****************************************************************
ok: [h1]
ok: [h2]
ok: [h3]
ok: [h4]
ok: [h5]

TASK [Create initial container setup from /home/pipi/net101/tools/netsim/ansible/templates/initial/linux-clab.j2] ***
changed: [h3 -> localhost]
changed: [h5 -> localhost]
changed: [h2 -> localhost]
changed: [h1 -> localhost]
changed: [h4 -> localhost]

TASK [Initial container configuration via /tmp/config-GTjzLkud-h1.sh] **********
changed: [h3 -> localhost]
changed: [h2 -> localhost]
changed: [h5 -> localhost]
changed: [h4 -> localhost]
changed: [h1 -> localhost]

TASK [file] ********************************************************************
changed: [h2 -> localhost]
changed: [h1 -> localhost]
changed: [h5 -> localhost]
changed: [h3 -> localhost]
changed: [h4 -> localhost]

TASK [Generate JSON-RPC YAML configuration] ************************************
ok: [r1]
ok: [r3]

TASK [Update SRL initial node configuration (template=/home/pipi/net101/tools/netsim/ansible/templates/initial/srlinux.j2)] ***
changed: [r1]
changed: [r3]

TASK [debug] *******************************************************************
skipping: [r1]
skipping: [r3]

PLAY [Deploy module-specific configurations] ***********************************

TASK [Set variables that cannot be set with VARS] ******************************
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Deploy individual configuration modules] *********************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for r1, r2, r3 => (item=vlan)
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for r1, r2, r3 => (item=ospf)
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-module.yml for r1, r2, r3 => (item=vrf)

TASK [Figure out whether to deploy the module vlan on current device] **********
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Find configuration template for vlan] ************************************
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [r2] => 
  msg: |-
    vlan configuration for r2
    =========================================
    #!/bin/bash
    #
    set -e # Exit immediately when any command fails
    #
    if [ ! -e /sys/devices/virtual/net/eth1.701 ]; then
      ip link add link eth1 name eth1.701 type vlan id 701
      ip link set dev eth1.701 up
    fi
    if [ ! -e /sys/devices/virtual/net/eth1.700 ]; then
      ip link add link eth1 name eth1.700 type vlan id 700
      ip link set dev eth1.700 up
    fi
    if [ ! -e /sys/devices/virtual/net/eth2.701 ]; then
      ip link add link eth2 name eth2.701 type vlan id 701
      ip link set dev eth2.701 up
    fi
    if [ ! -e /sys/devices/virtual/net/eth2.700 ]; then
      ip link add link eth2 name eth2.700 type vlan id 700
      ip link set dev eth2.700 up
    fi
  
    exit 0
ok: [r1] => 
  msg: |-
    vlan configuration for r1
    =========================================
  
    updates:
    - path: /interface[name=ethernet-1/2]
      value:
       subinterface:
       - index: 0
         type: routed
         description: "r1 ~ h1"
  
    - path: /interface[name=ethernet-1/2]/subinterface[index=0]
      value:
       description: "r1 ~ h1"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.2.1/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/2.0
  
    - path: /interface[name=ethernet-1/3]
      value:
       subinterface:
       - index: 0
         type: routed
         description: "r1 ~ h5"
  
    - path: /interface[name=ethernet-1/3]/subinterface[index=0]
      value:
       description: "r1 ~ h5"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.4.1/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/3.0
  
    - path: /interface[name=ethernet-1/4]
      value:
       subinterface:
       - index: 0
         type: routed
         description: "r1 ~ h3"
  
    - path: /interface[name=ethernet-1/4]/subinterface[index=0]
      value:
       description: "r1 ~ h3"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.5.1/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/4.0
  
    - path: /interface[name=ethernet-1/1]
      value:
       vlan-tagging: True
       subinterface:
       - index: 701
         type: routed
         description: "r1 ~ r2"
         vlan:
          encap:
           single-tagged:
            vlan-id: "701"
  
    - path: /interface[name=ethernet-1/1]/subinterface[index=701]
      value:
       description: "r1 ~ r2"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.7.1/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/1.701
  
    - path: /interface[name=ethernet-1/1]
      value:
       vlan-tagging: True
       subinterface:
       - index: 700
         type: routed
         description: "r1 ~ r2"
         vlan:
          encap:
           single-tagged:
            vlan-id: "700"
  
    - path: /interface[name=ethernet-1/1]/subinterface[index=700]
      value:
       description: "r1 ~ r2"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.8.1/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/1.700
ok: [r3] => 
  msg: |-
    vlan configuration for r3
    =========================================
  
    updates:
    - path: /interface[name=ethernet-1/2]
      value:
       subinterface:
       - index: 0
         type: routed
         description: "r3 ~ h2"
  
    - path: /interface[name=ethernet-1/2]/subinterface[index=0]
      value:
       description: "r3 ~ h2"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.3.3/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/2.0
  
    - path: /interface[name=ethernet-1/3]
      value:
       subinterface:
       - index: 0
         type: routed
         description: "r3 ~ h4"
  
    - path: /interface[name=ethernet-1/3]/subinterface[index=0]
      value:
       description: "r3 ~ h4"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.6.3/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/3.0
  
    - path: /interface[name=ethernet-1/1]
      value:
       vlan-tagging: True
       subinterface:
       - index: 701
         type: routed
         description: "r3 ~ r2"
         vlan:
          encap:
           single-tagged:
            vlan-id: "701"
  
    - path: /interface[name=ethernet-1/1]/subinterface[index=701]
      value:
       description: "r3 ~ r2"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.9.3/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/1.701
  
    - path: /interface[name=ethernet-1/1]
      value:
       vlan-tagging: True
       subinterface:
       - index: 700
         type: routed
         description: "r3 ~ r2"
         vlan:
          encap:
           single-tagged:
            vlan-id: "700"
  
    - path: /interface[name=ethernet-1/1]/subinterface[index=700]
      value:
       description: "r3 ~ r2"
       ipv4:
        admin-state: enable
        address:
        - ip-prefix: "172.16.10.3/24"
          primary: [null]
  
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
       interface:
       - name: ethernet-1/1.700

TASK [Find configuration deployment deploy_script for vlan] ********************
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Deploy vlan configuration] ***********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/srlinux.yml for r1, r3
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/frr.yml for r2

TASK [Generate JSON-RPC YAML configuration] ************************************
ok: [r1]
ok: [r3]

TASK [Update SRL vlan node configuration (template=/home/pipi/net101/tools/netsim/ansible/templates/vlan/srlinux.j2)] ***
changed: [r1]
changed: [r3]

TASK [debug] *******************************************************************
skipping: [r1]
skipping: [r3]

TASK [template] ****************************************************************
changed: [r2]

TASK [set_fact] ****************************************************************
ok: [r2]

TASK [run /tmp/config.sh to deploy vlan config from /home/pipi/net101/tools/netsim/ansible/templates/vlan/frr.j2] ***
changed: [r2]

TASK [run vtysh to import vlan config from /home/pipi/net101/tools/netsim/ansible/templates/vlan/frr.j2] ***
skipping: [r2]

TASK [Figure out whether to deploy the module ospf on current device] **********
ok: [r1]
ok: [r3]
ok: [r2]

TASK [Find configuration template for ospf] ************************************
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [r1] => 
  msg: |-
    ospf configuration for r1
    =========================================
    updates:
ok: [r2] => 
  msg: |-
    ospf configuration for r2
    =========================================
    !
    do write
ok: [r3] => 
  msg: |-
    ospf configuration for r3
    =========================================
    updates:

TASK [Find configuration deployment deploy_script for ospf] ********************
ok: [r1]
ok: [r3]
ok: [r2]

TASK [Deploy ospf configuration] ***********************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/srlinux.yml for r1, r3
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/frr.yml for r2

TASK [Generate JSON-RPC YAML configuration] ************************************
ok: [r1]
ok: [r3]

TASK [Update SRL ospf node configuration (template=/home/pipi/net101/tools/netsim/ansible/templates/ospf/srlinux.j2)] ***
skipping: [r1]
skipping: [r3]

TASK [debug] *******************************************************************
skipping: [r1]
skipping: [r3]

TASK [template] ****************************************************************
changed: [r2]

TASK [set_fact] ****************************************************************
ok: [r2]

TASK [run /tmp/config.sh to deploy ospf config from /home/pipi/net101/tools/netsim/ansible/templates/ospf/frr.j2] ***
skipping: [r2]

TASK [run vtysh to import ospf config from /home/pipi/net101/tools/netsim/ansible/templates/ospf/frr.j2] ***
changed: [r2]

TASK [Figure out whether to deploy the module vrf on current device] ***********
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Find configuration template for vrf] *************************************
ok: [r1]
ok: [r3]
ok: [r2]

TASK [Print deployed configuration when running in verbose mode] ***************
ok: [r2] => 
  msg: |-
    vrf configuration for r2
    =========================================
    #!/bin/bash
    #
    set -e # Exit immediately when any command fails
    #
  
    # Create VRF tables
    if [ ! -e /sys/devices/virtual/net/blue ]; then
    ip link add blue type vrf table 101
    fi
    ip link set blue up
    if [ ! -e /sys/devices/virtual/net/red ]; then
    ip link add red type vrf table 100
    fi
    ip link set red up
  
    # Move interfaces and loopbacks to vrfs
    sysctl -qw net.ipv6.conf.eth1.701.keep_addr_on_down=1
    ip link set eth1.701 master blue
    sysctl -qw net.ipv6.conf.eth1.700.keep_addr_on_down=1
    ip link set eth1.700 master red
    sysctl -qw net.ipv6.conf.eth2.701.keep_addr_on_down=1
    ip link set eth2.701 master blue
    sysctl -qw net.ipv6.conf.eth2.700.keep_addr_on_down=1
    ip link set eth2.700 master red
  
    cat >/tmp/vrf_config <<CONFIG
    vrf blue
     exit-vrf
    vrf red
     exit-vrf
    !
    router bgp 65000
    !
    !
    !
    ! OSPFv2 FRR configuration
    !
    router ospf vrf blue
     ospf router-id 10.0.0.2
     timers throttle spf 10 50 500
     timers throttle lsa all 100
     timers lsa min-arrival 100
  
     redistribute connected
  
    exit
    !
    interface eth1.701
    ! r2 -> r1
     ip ospf area 0.0.0.0
     ip ospf network point-to-point
    !
    interface eth2.701
    ! r2 -> r3
     ip ospf area 0.0.0.0
     ip ospf network point-to-point
    !
  
    !
    !
    ! OSPFv2 FRR configuration
    !
    router ospf vrf red
     ospf router-id 10.0.0.2
     timers throttle spf 10 50 500
     timers throttle lsa all 100
     timers lsa min-arrival 100
  
     redistribute connected
  
    exit
    !
    interface eth1.700
    ! r2 -> r1
     ip ospf area 0.0.0.0
     ip ospf network point-to-point
    !
    interface eth2.700
    ! r2 -> r3
     ip ospf area 0.0.0.0
     ip ospf network point-to-point
    !
  
    !
    do write
    !
    CONFIG
    vtysh -f /tmp/vrf_config
    exit $?
ok: [r1] => 
  msg: |-
    vrf configuration for r1
    =========================================
  
    updates:
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
  
    - path: /routing-policy/policy[name=blue_export_ospf]
      value:
        default-action:
          policy-result: reject
        statement:
        - name: export_local
          match:
            protocol: local
          action:
            policy-result: accept
  
  
    - path: /network-instance[name=blue]
      value:
        router-id: 10.0.0.1
        protocols:
          ospf:
            instance:
            - name: "0"
              version: ospf-v2
              admin-state: enable
              max-ecmp-paths: 64
              asbr: {}
              export-policy: "blue_export_ospf"
              area:
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/4.0
                  passive: True
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/1.701
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
  
  
    - path: /routing-policy/community-set[name=C65000_2]
      value:
       member:
       - "target:65000:2" # Single member, else matching is AND
  
    - path: /routing-policy/community-set[name=blue_export]
      value:
       member:
       - "target:65000:2"
  
  
    - path: /network-instance[name=blue]/protocols/bgp-vpn
      value:
       bgp-instance:
       - id: 1
         route-distinguisher:
          rd: "65000:2"
  
    - path: /network-instance[name=blue]/inter-instance-policies
      value:
       apply-policy:
        export-policy: "blue_vpn_export"
        import-policy: "blue_vpn_import"
  
    - path: /routing-policy/policy[name=blue_vpn_export]
      value:
       default-action:
        policy-result: "accept"
        bgp:
         communities:
          add: "blue_export"
  
    - path: /routing-policy/policy[name=blue_vpn_import]
      value:
       default-action:
        policy-result: "reject"
       statement:
       - name: 11
         match:
          bgp:
           community-set: "C65000_2"
         action:
          policy-result: "accept"
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
  
    - path: /routing-policy/policy[name=red_export_ospf]
      value:
        default-action:
          policy-result: reject
        statement:
        - name: export_local
          match:
            protocol: local
          action:
            policy-result: accept
  
  
    - path: /network-instance[name=red]
      value:
        router-id: 10.0.0.1
        protocols:
          ospf:
            instance:
            - name: "0"
              version: ospf-v2
              admin-state: enable
              max-ecmp-paths: 64
              asbr: {}
              export-policy: "red_export_ospf"
              area:
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/2.0
                  passive: True
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/3.0
                  passive: True
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/1.700
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
  
  
    - path: /routing-policy/community-set[name=C65000_1]
      value:
       member:
       - "target:65000:1" # Single member, else matching is AND
  
    - path: /routing-policy/community-set[name=red_export]
      value:
       member:
       - "target:65000:1"
  
  
    - path: /network-instance[name=red]/protocols/bgp-vpn
      value:
       bgp-instance:
       - id: 1
         route-distinguisher:
          rd: "65000:1"
  
    - path: /network-instance[name=red]/inter-instance-policies
      value:
       apply-policy:
        export-policy: "red_vpn_export"
        import-policy: "red_vpn_import"
  
    - path: /routing-policy/policy[name=red_vpn_export]
      value:
       default-action:
        policy-result: "accept"
        bgp:
         communities:
          add: "red_export"
  
    - path: /routing-policy/policy[name=red_vpn_import]
      value:
       default-action:
        policy-result: "reject"
       statement:
       - name: 11
         match:
          bgp:
           community-set: "C65000_1"
         action:
          policy-result: "accept"
ok: [r3] => 
  msg: |-
    vrf configuration for r3
    =========================================
  
    updates:
  
    - path: /network-instance[name=blue]
      value:
       type: ip-vrf
  
    - path: /routing-policy/policy[name=blue_export_ospf]
      value:
        default-action:
          policy-result: reject
        statement:
        - name: export_local
          match:
            protocol: local
          action:
            policy-result: accept
  
  
    - path: /network-instance[name=blue]
      value:
        router-id: 10.0.0.3
        protocols:
          ospf:
            instance:
            - name: "0"
              version: ospf-v2
              admin-state: enable
              max-ecmp-paths: 64
              asbr: {}
              export-policy: "blue_export_ospf"
              area:
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/3.0
                  passive: True
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/1.701
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
  
  
    - path: /routing-policy/community-set[name=C65000_2]
      value:
       member:
       - "target:65000:2" # Single member, else matching is AND
  
    - path: /routing-policy/community-set[name=blue_export]
      value:
       member:
       - "target:65000:2"
  
  
    - path: /network-instance[name=blue]/protocols/bgp-vpn
      value:
       bgp-instance:
       - id: 1
         route-distinguisher:
          rd: "65000:2"
  
    - path: /network-instance[name=blue]/inter-instance-policies
      value:
       apply-policy:
        export-policy: "blue_vpn_export"
        import-policy: "blue_vpn_import"
  
    - path: /routing-policy/policy[name=blue_vpn_export]
      value:
       default-action:
        policy-result: "accept"
        bgp:
         communities:
          add: "blue_export"
  
    - path: /routing-policy/policy[name=blue_vpn_import]
      value:
       default-action:
        policy-result: "reject"
       statement:
       - name: 11
         match:
          bgp:
           community-set: "C65000_2"
         action:
          policy-result: "accept"
  
  
    - path: /network-instance[name=red]
      value:
       type: ip-vrf
  
    - path: /routing-policy/policy[name=red_export_ospf]
      value:
        default-action:
          policy-result: reject
        statement:
        - name: export_local
          match:
            protocol: local
          action:
            policy-result: accept
  
  
    - path: /network-instance[name=red]
      value:
        router-id: 10.0.0.3
        protocols:
          ospf:
            instance:
            - name: "0"
              version: ospf-v2
              admin-state: enable
              max-ecmp-paths: 64
              asbr: {}
              export-policy: "red_export_ospf"
              area:
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/2.0
                  passive: True
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
              - area-id: 0.0.0.0
                interface:
                - interface-name: ethernet-1/1.700
                  interface-type: "point-to-point"
                  failure-detection:
                    enable-bfd: False
  
  
    - path: /routing-policy/community-set[name=C65000_1]
      value:
       member:
       - "target:65000:1" # Single member, else matching is AND
  
    - path: /routing-policy/community-set[name=red_export]
      value:
       member:
       - "target:65000:1"
  
  
    - path: /network-instance[name=red]/protocols/bgp-vpn
      value:
       bgp-instance:
       - id: 1
         route-distinguisher:
          rd: "65000:1"
  
    - path: /network-instance[name=red]/inter-instance-policies
      value:
       apply-policy:
        export-policy: "red_vpn_export"
        import-policy: "red_vpn_import"
  
    - path: /routing-policy/policy[name=red_vpn_export]
      value:
       default-action:
        policy-result: "accept"
        bgp:
         communities:
          add: "red_export"
  
    - path: /routing-policy/policy[name=red_vpn_import]
      value:
       default-action:
        policy-result: "reject"
       statement:
       - name: 11
         match:
          bgp:
           community-set: "C65000_1"
         action:
          policy-result: "accept"

TASK [Find configuration deployment deploy_script for vrf] *********************
ok: [r1]
ok: [r2]
ok: [r3]

TASK [Deploy vrf configuration] ************************************************
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/srlinux.yml for r1, r3
included: /home/pipi/net101/tools/netsim/ansible/tasks/deploy-config/frr.yml for r2

TASK [Generate JSON-RPC YAML configuration] ************************************
ok: [r1]
ok: [r3]

TASK [Update SRL vrf node configuration (template=/home/pipi/net101/tools/netsim/ansible/templates/vrf/srlinux.j2)] ***
changed: [r3]
changed: [r1]

TASK [debug] *******************************************************************
skipping: [r1]
skipping: [r3]

TASK [template] ****************************************************************
changed: [r2]

TASK [set_fact] ****************************************************************
ok: [r2]

TASK [run /tmp/config.sh to deploy vrf config from /home/pipi/net101/tools/netsim/ansible/templates/vrf/frr.j2] ***
changed: [r2]

TASK [run vtysh to import vrf config from /home/pipi/net101/tools/netsim/ansible/templates/vrf/frr.j2] ***
skipping: [r2]

PLAY [Deploy custom deployment templates] **************************************
skipping: no hosts matched

PLAY RECAP *********************************************************************
h1                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
h2                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
h3                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
h4                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
h5                         : ok=12   changed=3    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
r1                         : ok=34   changed=3    unreachable=0    failed=0    skipped=6    rescued=0    ignored=0   
r2                         : ok=41   changed=9    unreachable=0    failed=0    skipped=6    rescued=0    ignored=0   
r3                         : ok=34   changed=3    unreachable=0    failed=0    skipped=6    rescued=0    ignored=0   



VRF lite implementation with VLAN trunks

* h1, h2, and h5 should be able to ping each other
* h3 and h4 should be able to ping each other

Please note it might take a while for the lab to work due to
STP and OSPF setup phase

